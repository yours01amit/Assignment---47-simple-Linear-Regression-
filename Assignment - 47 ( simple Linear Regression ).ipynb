{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple Linear Regression\n",
    "  - A statistical technique that uses single explanatory variable to predict the outcome of a response variable.\n",
    "  - One dependent variable Y predicted from one independent variable X.\n",
    "  - Only one regression coefficient.\n",
    "  - r^2 : Proportion of varation in dependent varable Y predictable from X.\n",
    "  - Example:\n",
    "    - X = No of hours studying per day\n",
    "    - Y = Marks in the exam\n",
    "\n",
    "#\n",
    "\n",
    "- Multiple Linear Regression\n",
    "  - A statistical technique that uses multiple explanatory variables to predict the outcome of a response variable.\n",
    "  - One dependent variable Y predicted from a set of independent variables (X1, X2... Xn).\n",
    "  - One regression coefficient for each independent variable.\n",
    "  - r^2 : Proportion of varation in dependent varable Y predictable from by a set of independent variables(X's).\n",
    "  - Example:\n",
    "    - X1 = Years of experience\n",
    "    - X2 = Level of education\n",
    "    - Y = Salary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are primarily five assumptions of linear regression.\n",
    "- They are:\n",
    "    - Linear Relationship:\n",
    "        - There is a linear relationship between the predictors (X) and the outcome (Y)\n",
    "    - Independence\n",
    "        - Predictors (X) are independent and observed with negligible error\n",
    "    - Normality\n",
    "        - Residual Errors have a mean value of zero\n",
    "    - Homoscedasticity\n",
    "        - Residual Errors have constant variance\n",
    "    - No multicollinearity\n",
    "        - Residual Errors are independent from each other and predictors (X)\n",
    "#\n",
    "- To check whether these assumptions hold in a given dataset, we can perform\n",
    "    - Scatterplot visualisation\n",
    "    - residual Plot\n",
    "    - Correlation Matrix\n",
    "    - Statistical Tests like Shapiro-Wilk test, Breusch-Pagan test, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The slope in a linear regression can be interpreted as:\n",
    "    - Slope is a ratio of the change in one variable to the change in the other.\n",
    "    - If the slope is positive, it indicates that the relationship between the independent and dependent variables is positive, meaning that as x increases, y also increases.\n",
    "    - If the slope is negative, it indicates that the relationship is negative, meaning that as x increases, y decreases.\n",
    "    #\n",
    "- The intercept can be interpreted as: \n",
    "    - The predicted value of y when x has value zero.\n",
    "    - In some cases, the intercept may not have a meaningful interpretation, particularly if the independent variable cannot take on a value of zero in the real world.\n",
    "#\n",
    "- Example:\n",
    "    - Cab fare\n",
    "        - Let the cost for booking a cab is Rs. 200 then for each kilometer of travel, the driver charges Rs. 20/km.\n",
    "            - So the equation for the cost of booking the cab will be:\n",
    "                - Fare = Booking_Cost + 20 * (KM_travelled)\n",
    "        - In the above example, the intercept is 200 and the slope is 20, meaning the increase in fare for each kilometer of travel.\n",
    "        - Therefore, we can interpret the results as follows: traveller who travels more kilometer tend to pay higher fare, and for every additional kilometer travelled, we expect to see a factor of 20 increase in fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient descent is an optimization algorithm used in machine learning to minimize the cost function of a model by iteratively adjusting its parameters in the opposite direction of the gradient.\n",
    "- The gradient is the slope of the cost function, and by moving in the direction of the negative gradient, the algorithm can converge to the optimal set of parameters that best fits the training data.\n",
    "- Gradient descent can be applied to a wide range of machine learning algorithms, including linear regression, logistic regression, neural networks, and support vector machines.\n",
    "-  It helps in finding the local minimum of a function that is to optimize the parameters of a model in order to minimize the error between the predicted and actual values.\n",
    "#\n",
    "- Gradient descent, in machine learning, is used to optimize the parameters of a model by minimizing a cost function, which measures the error between the predicted and actual values.\n",
    "- Example\n",
    "    - In linear regression, gradient descent can be used to find the values of the slope and intercept that minimize the sum of squared errors between the predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiple Linear Regression\n",
    "  - A statistical technique that uses multiple explanatory variables to predict the outcome of a response variable.\n",
    "  - One dependent variable Y predicted from a set of independent variables (X1, X2... Xn).\n",
    "  - One regression coefficient for each independent variable.\n",
    "  - r^2 : Proportion of varation in dependent varable Y predictable from by a set of independent variables(X's).\n",
    "  - Example:\n",
    "    - X1 = Years of experience\n",
    "    - X2 = Level of education\n",
    "    - Y = Salary\n",
    "#\n",
    "- But a Simple Linear Regression has follwoing features:\n",
    "  - A statistical technique that uses single explanatory variable to predict the outcome of a response variable.\n",
    "  - One dependent variable Y predicted from one independent variable X.\n",
    "  - Only one regression coefficient.\n",
    "  - r^2 : Proportion of varation in dependent varable Y predictable from X.\n",
    "  - Example:\n",
    "    - X = No of hours studying per day\n",
    "    - Y = Marks in the exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multicollinearity occurs while conducting multiple linear regression and can pose a common issue when two or more independent variables are highly correlated with one another.\n",
    "- This results in redundant information being provided by the independent variables, which can cause difficulties in accurately determining the true relationship between the independent and dependent variables, as well as interpreting the coefficients of the independent variables.\n",
    "#\n",
    "- Detection of multicollinearity\n",
    "    - It can be done through calculating the correlation matrix between independent variables.\n",
    "    - If two or more of these variables have a high correlation (greater than 0.7), then there may be an issue with multicollinearity.\n",
    "    - Another way to detect it is by using variance inflation factor (VIF) for each variable; if VIF is greater than 5 or 10, then multicollinearity may be present.\n",
    "#\n",
    "- Addressing multicollinearity:\n",
    "    - Remove one of the correlated independent variables\n",
    "        - If two or more of these variables have high correlation, removing one from the model could reduce multicollinearity.\n",
    "    - Combine correlated independent variables\n",
    "        - If these correlated variables are conceptually related, they could be combined into a single variable using methods such as factor analysis or principal component analysis.\n",
    "    - Use regularization techniques\n",
    "        - Regularization techniques like ridge regression and lasso regression can penalize large coefficients and reduce multicollinearity's impact.\n",
    "    - Collect more data\n",
    "        - Collecting more data helps increase variation in independent variables and reduces multicollinearity's impact.\n",
    "#\n",
    "- By addressing this issue effectively, we can improve accuracy and reliability while making precise predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In polynomial regression, the relationship between the independent variable x and the dependent variable y is described as an nth degree polynomial in x.\n",
    "- It is abbreviated E(y |x), describes the fitting of a nonlinear relationship between the value of x and the conditional mean of y. It usually corresponded to the least-squares method which minimizes the variance of the coefficients.\n",
    "- The general form of a polynomial regression model is:\n",
    "    - y = b0 + b1x + b2x^2 + ... + bnx^n + Îµ\n",
    "#\n",
    "- When it comes to regression analysis, there is a key distinction between polynomial regression and linear regression.\n",
    "    - While linear regression can only model linear relationships or straight line relationships between independent and dependent variables, polynomial regression is capable of modeling nonlinear(curved or U-shaped relationships as well as inverted U-shaped ones)relationships making it a more versatile tool for analyzing data with complex patterns.\n",
    "- Polynomial regression can lead to overfitting if the degree of the polynomial is too high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The advantage of the polynomial regressssion compared to linear regression is:\n",
    "    - Polynomial regression works on nonlinear relationships between the independent and dependent variables, while linear regression only works on linear relationships.\n",
    "    - Polynomial regression is more flexible than linear regression as it can fit a wider range of data patterns and can be customized to fit a specific set of data.\n",
    "#\n",
    "- The disadvantage of the polynomial regressssion compared to linear regression is:\n",
    "    - Polynoimial regression can lead to overfittting if the degree of the polynomial is high.\n",
    "    - Polynomial regression is more complex than linear regression as it involves estimating multiple coefficients for the polynomial terms.\n",
    "#\n",
    "- Polynimial regression is desirable\n",
    "    - When the relationship between the independent and dependent variables is known to be nonlinear\n",
    "    - When linear regression fails to adequately capture the relationship between the independent and dependent variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
